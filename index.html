<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>VL-SAFE: Vision-Language Guided Safety-Aware Reinforcement Learning with World Models for Autonomous Driving</title>
  <link rel="icon" type="image/x-icon" href="static/images/Icon.jpg">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">VL-SAFE: Vision-Language Guided Safety-Aware Reinforcement Learning with World Models for Autonomous Driving</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=hIt7KnUAAAAJ&hl=zh-CN&oi=sra" target="_blank">Yansong Qu</a><sup>†</sup>,</span>
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?user=RgO7ppoAAAAJ&hl=zh-CN&oi=ao" target="_blank">Zilin Huang</a><sup>†</sup>,</span>
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=3T-SILsAAAAJ&hl=zh-CN&oi=ao" target="_blank">Zihao Sheng</a><sup>†</sup>,
                  </span>
                  <span class="author-block">
                    <a href="FOURTH AUTHOR PERSONAL LINK" target="_blank">Jiancong Chen</a>,
                  </span>
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=hCXBQl8AAAAJ&hl=zh-CN&oi=ao" target="_blank">Samuel Labi</a><sup>*</sup>,
                  </span>
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=DPN2wc4AAAAJ&hl=zh-CN&oi=ao" target="_blank">Sikai Chen</a><sup>*</sup>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">
                      Purdue University – West Lafayette<br>
                      University of Wisconsin – Madison<br>
                      Conference name and year
                    </span>
                    <span class="eql-cntrb"><small><br><sup>†</sup>Indicates Equal Contribution</small></span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Corresponding Author</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/<ARXIV PAPER ID>.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/ys-qu/vl-safe/tree/main" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- Video Link -->
                <span class="link-block">
                      <a href="https://www.youtube.com/watch?v=cL4K3Fshjxk" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-video"></i> <!-- Video icon -->
                      </span>
                      <span>Video</span>
                    </a>
                  </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Reinforcement learning (RL)-based autonomous driving policy learning faces critical limitations such as low sample efficiency and poor generalization; its reliance on online interactions and trial-and-error learning is especially unacceptable in safety-critical scenarios. Existing methods, such as safe RL, often fail to capture the true semantic meaning of "safety" in complex driving contexts, leading to either overly conservative behavior or constraint violations. To address these challenges, we propose VL-SAFE, a world model-based safe RL framework with Vision-Language model (VLM)-as safety guidance paradigm, designed for offline safe policy learning. Specifically, we construct offline datasets collected by expert agents and labeled with safety scores derived from VLMs. A world model is trained to generate imagined rollouts together with safety estimations, allowing the agent to perform safe planning without interacting with the real environment. Based on these imagined trajectories and safety evaluations, actor-critic learning is conducted under VLM-based safety guidance to optimize the driving policy more safely and efficiently. Extensive evaluations demonstrate that VL-SAFE achieves superior sample efficiency, generalization, safety, and overall performance compared to existing baselines. To the best of our knowledge, this is the first work that introduces a VLM-guided world model-based approach for safe autonomous driving.  The demo video and code can be accessed at: <a href="https://ys-qu.github.io/vlsafe-website/" target="_blank">https://ys-qu.github.io/vlsafe-website/</a>
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Single Image with Title and Shadowed Container -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container" style="max-width: 990px; margin: auto; box-shadow: 0px 4px 10px rgba(0, 0, 0, 0.1); padding: 20px; background-color: white; border-radius: 8px;">
      <h2 class="title is-3" style="font-size: 24px; text-align: left;">Comparisons between our proposed method and related work.</h2> <!-- Title for the image -->
      <br>
      <div class="image-container has-text-centered" style="max-width: 100%; margin: auto;">
        <!-- Your image here with adjusted width -->
        <img src="static/images/_intro_comparison.png" alt="Descriptive Alt Text" style="width: 100%; height: auto;"/>
      </div>
      <div class="subtitle-container" style="max-width: 100%; margin: auto;">
        <h2 class="subtitle" style="font-size: 15px; text-align: justify;">
          <p>
          (a) Offline RL, removes the need for online exploration entirely by learning from pre-collected driving data, thus avoiding the safety concerns of real-time trial-and-error; but it still fails occasionally in online testing due to no safety constraints.
          </p>
          <p>
          (b) World models can reduce interactions by enabling policy learning through imagined rollouts, this can largely reduce risky actions and improve sample efficiency; but world models can still make risky actions at the initial training stage.
          </p>
          <p>
          (c) Safe RL methods explicitly incorporate cost constraints or risk-sensitive objectives into the learning process to restrict unsafe behaviors and ensure long-term safety; safe RL might learn a too conservative policy and unable to generalize due to the lack of semantic understanding of the whole context.
          </p>
          <p>
          (d) It is intuitive to combine these methods together to complement each other's strengths and weaknesses. However, more importantly, at the core of these challenges lies a fundamental question: <strong>how can we identify risky states, semantically understand “safety”, and guide policy learning accordingly?</strong>
          </p>
        </h2>
      </div>
    </div>
  </div>
</section>

<!-- Single Image with Title and Shadowed Container -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container" style="max-width: 990px; margin: auto; box-shadow: 0px 4px 10px rgba(0, 0, 0, 0.1); padding: 20px; background-color: white; border-radius: 8px;">
      <h2 class="title is-3" style="font-size: 24px; text-align: left;">An example from life to prove the effectiveness of VLM-as safety guidance paradigm.</h2> <!-- Title for the image -->
      <br>
      <div class="image-container has-text-centered" style="max-width: 100%; margin: auto;">
        <!-- Your image here with adjusted width -->
        <img src="static/images/motivation.png" alt="Descriptive Alt Text" style="width: 100%; height: auto;"/>
      </div>
      <div class="subtitle-container" style="max-width: 100%; margin: auto;">
        <h2 class="subtitle" style="font-size: 15px; text-align: justify;">
          <p>
          We introduce our motivation starting from an intuitive life example: imagine an autonomous vehicle approaching a stopped school bus on a multi-lane road, where several children are walking across the street toward the sidewalk. The school bus has its STOP sign extended, indicating that all surrounding vehicles must come to a complete stop.
          </p>
          <p>
          A classical RL-based or offline-trained agent may continue driving or only slightly slow down, as no collision has occurred and the reward function still favors progress. A world model-based agent may also ignore the school bus and its STOP sign during imagined rollouts, failing to distinguish the risk embedded in this visual context. A traditional safe RL policy may behave unpredictably: it may stop in some cases, but often fails to recognize the raised STOP sign and the presence of children, especially if such visual cues were not explicitly encoded in the cost function or training data. This can lead to unsafe decisions or over-generalized conservativeness, such as treating harmless objects like parked vehicles or cones as equivalent threats.
          </p>
          <p>
          In contrast, a <strong>VLM-guided policy</strong> can semantically understand the scene: the raised STOP sign on the school bus and the crossing children clearly indicate a high-risk situation. Even in the absence of a collision, the VLM assigns a low safety score based on visual semantics, guiding the agent to stop proactively and yield to the pedestrians. This enables the agent to maintain appropriate caution in dangerous scenarios without falling into the trap of indiscriminate conservativeness.
          </p>
          <p>
          This example highlights the advantage of <strong>semantic safety reasoning</strong>: it enables agents to make early, context-aware, and proportionate decisions, ultimately allowing them to act safely across varied scenarios.
          </p>
        </h2>
      </div>
    </div>
  </div>
</section>

<!-- Single Image with Title and Shadowed Container -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container" style="max-width: 990px; margin: auto; box-shadow: 0px 4px 10px rgba(0, 0, 0, 0.1); padding: 20px; background-color: white; border-radius: 8px;">
      <h2 class="title is-3" style="font-size: 24px; text-align: left;">Overall framework of VL-SAFE.</h2> <!-- Title for the image -->
      <br>
      <div class="image-container has-text-centered" style="max-width: 100%; margin: auto;">
        <!-- Your image here with adjusted width -->
        <img src="static/images/_overall_framework.png" alt="Descriptive Alt Text" style="width: 100%; height: auto;"/>
      </div>
      <div class="subtitle-container" style="max-width: 100%; margin: auto;">
        <h2 class="subtitle" style="font-size: 15px; text-align: justify;">
          <p>
          The proposed method VL-SAFE has two phases, where the first phase generates ground truth safety estimations using CLIPs for each state in offline dataset collected by expert agent, and the second stage will learn a safety-aware world model to generate imagined rollouts along with predicted safety estimations for actor-critic learning.
          </p>
          <p>
          In our approach, we similarly assign weights to each state-action pair in the imagined rollouts. The weight term combines both reward and cost advantages, and is modulated by a VLM-derived safety probability, which dynamically balances the influence of reward-seeking and cost-avoidance. The intuition is that the agent should prioritize reward maximization in safe conditions, and focus on cost minimization when encountering potentially risky situations, where the timing and degree of this tradeoff is guided by the semantic understanding provided by the VLM.
          </p>
        </h2>
      </div>
    </div>
  </div>
</section>

<!-- Single Image with Title and Shadowed Container -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container" style="max-width: 990px; margin: auto; box-shadow: 0px 4px 10px rgba(0, 0, 0, 0.1); padding: 20px; background-color: white; border-radius: 8px;">
      <h2 class="title is-3" style="font-size: 24px; text-align: left;">Driving scenarios and maps.</h2> <!-- Title for the image -->
      <br>
      <div class="image-container has-text-centered" style="max-width: 100%; margin: auto;">
        <!-- Your image here with adjusted width -->
        <img src="static/images/ds2.png" alt="Descriptive Alt Text" style="width: 100%; height: auto;"/>
      </div>
      <div class="subtitle-container" style="max-width: 100%; margin: auto;">
        <h2 class="subtitle" style="font-size: 15px; text-align: justify;">
          All tasks in the CarDreamer simulation platform are conducted on Town 03 and Town 04 maps under CARLA, which provide realistic urban and suburban layouts with diverse traffic elements. As shown in Figure above, the scenarios cover various road geometries and traffic situations essential for testing generalization and robustness in autonomous policies.
        </h2>
      </div>
    </div>
  </div>
</section>

<!-- Single Image with Title and Shadowed Container -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container" style="max-width: 990px; margin: auto; box-shadow: 0px 4px 10px rgba(0, 0, 0, 0.1); padding: 20px; background-color: white; border-radius: 8px;">
      <h2 class="title is-3" style="font-size: 24px; text-align: left;">Comparison results.</h2> <!-- Title for the image -->
      <br>
      <div class="image-container has-text-centered" style="max-width: 100%; margin: auto;">
        <!-- Your image here with adjusted width -->
        <img src="static/images/comp_res.png" alt="Descriptive Alt Text" style="width: 100%; height: auto;"/>
      </div>
      <div class="subtitle-container" style="max-width: 100%; margin: auto;">
        <h2 class="subtitle" style="font-size: 15px; text-align: justify;">
          Compared to traditional methods, our framework yields superior performance in terms of driving safety, sampling efficiency, and generalizability.
        </h2>
      </div>
    </div>
  </div>
</section>

<!-- Youtube video -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video">
            <!-- Youtube embed code here -->
            <iframe src="https://www.youtube.com/embed/cL4K3Fshjxk" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End youtube video -->

<!-- Paper poster -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/poster - Yansong Qu - Apr 24.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section>
<!--End paper poster -->

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
